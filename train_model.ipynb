{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HUFFINGTON POST DATASET CATEGORY LABEL ML MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use json module to load the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(r'F:\\notes\\News_Category_Dataset.json\\News_Category_Dataset.json') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_list = []\n",
    "headline_list = []\n",
    "description_list = []\n",
    "author_list = []\n",
    "label_list = []\n",
    "all_text = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create separate lists for each attribute and fill them with all the data for subsequent attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in data:\n",
    "    category_list.append(f.get('category'))\n",
    "    headline_list.append(f.get('headline'))\n",
    "    description_list.append(f.get('short_description'))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category: ENTERTAINMENT headline: Will Smith Joins Diplo And Nicky Jam For The 2018 World Cup's Official Song description: Of course it has a song.\n"
     ]
    }
   ],
   "source": [
    "print(\"category:\",category_list[1],\n",
    "\"headline:\",headline_list[1],\n",
    "\"description:\",description_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {'POLITICS': 1,\n",
    "'ENTERTAINMENT': 2,\n",
    "'HEALTHY LIVING': 3,\n",
    "'QUEER VOICES': 4,\n",
    "'BUSINESS': 5,\n",
    "'SPORTS': 6,\n",
    "'COMEDY': 7,\n",
    "'PARENTS': 8,\n",
    "'BLACK VOICES': 9,\n",
    "'THE WORLDPOST': 10,\n",
    "'WOMEN': 11,\n",
    "'CRIME': 12,\n",
    "'MEDIA': 13,\n",
    "'WEIRD NEWS': 14,\n",
    "'GREEN': 15,\n",
    "'IMPACT': 16,\n",
    "'WORLDPOST': 17,\n",
    "'RELIGION': 18,\n",
    "'STYLE': 19,\n",
    "'WORLD NEWS': 20,\n",
    "'TRAVEL': 21,\n",
    "'TASTE': 22,\n",
    "'ARTS': 23,\n",
    "'FIFTY': 24,\n",
    "'GOOD NEWS': 25,\n",
    "'SCIENCE': 26,\n",
    "'ARTS & CULTURE': 27,\n",
    "'TECH': 28,\n",
    "'COLLEGE': 29,\n",
    "'LATINO VOICES': 30,\n",
    "'EDUCATION': 31}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, Labels i.e. category  are in simple integer \n",
    ",we need to convert it into one hot encode so the model can understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(dictionary,number):\n",
    "    \n",
    "    for f in dictionary:\n",
    "        out = np.zeros(number)\n",
    "        n = dictionary.get(f)\n",
    "        out[n-1] = 1\n",
    "        dictionary[f] = out\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'POLITICS': array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'ENTERTAINMENT': array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'HEALTHY LIVING': array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'QUEER VOICES': array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'BUSINESS': array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'SPORTS': array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'COMEDY': array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'PARENTS': array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'BLACK VOICES': array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'THE WORLDPOST': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'WOMEN': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'CRIME': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'MEDIA': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'WEIRD NEWS': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'GREEN': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'IMPACT': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'WORLDPOST': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'RELIGION': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'STYLE': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'WORLD NEWS': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'TRAVEL': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'TASTE': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'ARTS': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'FIFTY': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'GOOD NEWS': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]),\n",
       " 'SCIENCE': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
       " 'ARTS & CULTURE': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]),\n",
       " 'TECH': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " 'COLLEGE': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]),\n",
       " 'LATINO VOICES': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " 'EDUCATION': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out=one_hot_encoder(labels,31)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use this encoder function that i made or you can use tflearn and keras built-in functions to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in category_list:\n",
    "    label_list.append(out.get(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets convert our category list into one hot encoded list and store in label_ist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i ,j in zip(description_list, headline_list):\n",
    "    all_text.append(i+j)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now , description and headline are separate.Lets concatenate them so that model can train both on headlines as well as description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GRU, Embedding,LSTM\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'to': 2,\n",
       " 'a': 3,\n",
       " 'of': 4,\n",
       " 'and': 5,\n",
       " 'in': 6,\n",
       " 'is': 7,\n",
       " 'for': 8,\n",
       " 'on': 9,\n",
       " 'that': 10,\n",
       " 'with': 11,\n",
       " 'it': 12,\n",
       " 'you': 13,\n",
       " 'are': 14,\n",
       " 'i': 15,\n",
       " 'this': 16,\n",
       " 'be': 17,\n",
       " 'as': 18,\n",
       " 'at': 19,\n",
       " 'trump': 20,\n",
       " 'from': 21,\n",
       " 'have': 22,\n",
       " 'not': 23,\n",
       " 'we': 24,\n",
       " 'about': 25,\n",
       " 'was': 26,\n",
       " 'new': 27,\n",
       " 'his': 28,\n",
       " 'but': 29,\n",
       " 'your': 30,\n",
       " 'has': 31,\n",
       " 'an': 32,\n",
       " 'what': 33,\n",
       " 'will': 34,\n",
       " 'by': 35,\n",
       " 'he': 36,\n",
       " 'my': 37,\n",
       " 'one': 38,\n",
       " 'all': 39,\n",
       " 'how': 40,\n",
       " 'more': 41,\n",
       " 'who': 42,\n",
       " 'can': 43,\n",
       " 'their': 44,\n",
       " 'up': 45,\n",
       " 'people': 46,\n",
       " 'after': 47,\n",
       " 'out': 48,\n",
       " 'our': 49,\n",
       " 'her': 50,\n",
       " 'just': 51,\n",
       " 'they': 52,\n",
       " 'when': 53,\n",
       " 'or': 54,\n",
       " 'donald': 55,\n",
       " \"it's\": 56,\n",
       " 'like': 57,\n",
       " 'if': 58,\n",
       " 'time': 59,\n",
       " 'over': 60,\n",
       " 'so': 61,\n",
       " 'do': 62,\n",
       " 'no': 63,\n",
       " 'says': 64,\n",
       " 'than': 65,\n",
       " 'said': 66,\n",
       " 'why': 67,\n",
       " 'year': 68,\n",
       " 'first': 69,\n",
       " 'been': 70,\n",
       " 's': 71,\n",
       " 'day': 72,\n",
       " 'u': 73,\n",
       " 'president': 74,\n",
       " 'get': 75,\n",
       " 'world': 76,\n",
       " 'women': 77,\n",
       " 'life': 78,\n",
       " 'some': 79,\n",
       " 'most': 80,\n",
       " 'now': 81,\n",
       " 'make': 82,\n",
       " 'into': 83,\n",
       " 'would': 84,\n",
       " 'us': 85,\n",
       " 'could': 86,\n",
       " 'there': 87,\n",
       " 'its': 88,\n",
       " 'she': 89,\n",
       " 'me': 90,\n",
       " 'years': 91,\n",
       " 'these': 92,\n",
       " 'know': 93,\n",
       " 'should': 94,\n",
       " 'were': 95,\n",
       " 'may': 96,\n",
       " 'had': 97,\n",
       " 'way': 98,\n",
       " 'need': 99,\n",
       " \"trump's\": 100,\n",
       " 'say': 101,\n",
       " \"don't\": 102,\n",
       " 'them': 103,\n",
       " 'back': 104,\n",
       " 'many': 105,\n",
       " 'being': 106,\n",
       " 'two': 107,\n",
       " 'even': 108,\n",
       " 'state': 109,\n",
       " 'want': 110,\n",
       " 'against': 111,\n",
       " 'house': 112,\n",
       " 'still': 113,\n",
       " 'white': 114,\n",
       " 'love': 115,\n",
       " 'help': 116,\n",
       " 'good': 117,\n",
       " 'week': 118,\n",
       " 'only': 119,\n",
       " 'other': 120,\n",
       " 'clinton': 121,\n",
       " 'health': 122,\n",
       " 'man': 123,\n",
       " 'down': 124,\n",
       " 'take': 125,\n",
       " 'best': 126,\n",
       " 'because': 127,\n",
       " 'old': 128,\n",
       " 'news': 129,\n",
       " 'gop': 130,\n",
       " 'right': 131,\n",
       " 'last': 132,\n",
       " 'off': 133,\n",
       " 'black': 134,\n",
       " 'change': 135,\n",
       " 'police': 136,\n",
       " 'obama': 137,\n",
       " 'him': 138,\n",
       " 'really': 139,\n",
       " 'american': 140,\n",
       " 'things': 141,\n",
       " 'show': 142,\n",
       " 'think': 143,\n",
       " 'before': 144,\n",
       " 'much': 145,\n",
       " 'never': 146,\n",
       " 'too': 147,\n",
       " 'see': 148,\n",
       " 'while': 149,\n",
       " 'go': 150,\n",
       " 'every': 151,\n",
       " 'work': 152,\n",
       " 'bill': 153,\n",
       " 'hillary': 154,\n",
       " 'also': 155,\n",
       " 'school': 156,\n",
       " 'big': 157,\n",
       " 'made': 158,\n",
       " 'going': 159,\n",
       " 'those': 160,\n",
       " 'here': 161,\n",
       " 'family': 162,\n",
       " 'own': 163,\n",
       " 'kids': 164,\n",
       " 'campaign': 165,\n",
       " 'look': 166,\n",
       " 'america': 167,\n",
       " 'where': 168,\n",
       " 'care': 169,\n",
       " 'long': 170,\n",
       " 'during': 171,\n",
       " 'woman': 172,\n",
       " 'court': 173,\n",
       " 'video': 174,\n",
       " 'better': 175,\n",
       " 'election': 176,\n",
       " '5': 177,\n",
       " 'former': 178,\n",
       " 'very': 179,\n",
       " 'ever': 180,\n",
       " 'which': 181,\n",
       " 'another': 182,\n",
       " 'watch': 183,\n",
       " 'through': 184,\n",
       " 'real': 185,\n",
       " 'little': 186,\n",
       " \"here's\": 187,\n",
       " 'gay': 188,\n",
       " '10': 189,\n",
       " 'climate': 190,\n",
       " 'did': 191,\n",
       " 'war': 192,\n",
       " 'death': 193,\n",
       " 'media': 194,\n",
       " 'country': 195,\n",
       " 'americans': 196,\n",
       " 'got': 197,\n",
       " 'any': 198,\n",
       " 'well': 199,\n",
       " 'children': 200,\n",
       " 'national': 201,\n",
       " 'stop': 202,\n",
       " 'states': 203,\n",
       " 'home': 204,\n",
       " 'again': 205,\n",
       " 'republican': 206,\n",
       " 'next': 207,\n",
       " 'rights': 208,\n",
       " 'wants': 209,\n",
       " 'star': 210,\n",
       " 'york': 211,\n",
       " 'end': 212,\n",
       " 'political': 213,\n",
       " 'great': 214,\n",
       " 'party': 215,\n",
       " 'part': 216,\n",
       " 'around': 217,\n",
       " 'something': 218,\n",
       " \"doesn't\": 219,\n",
       " \"can't\": 220,\n",
       " 'men': 221,\n",
       " '1': 222,\n",
       " 'since': 223,\n",
       " 'history': 224,\n",
       " 'days': 225,\n",
       " 'public': 226,\n",
       " 'city': 227,\n",
       " 'republicans': 228,\n",
       " \"i'm\": 229,\n",
       " 'does': 230,\n",
       " 'million': 231,\n",
       " 'come': 232,\n",
       " 'anti': 233,\n",
       " 'might': 234,\n",
       " 'high': 235,\n",
       " 'story': 236,\n",
       " 'must': 237,\n",
       " 'thing': 238,\n",
       " 'law': 239,\n",
       " 'business': 240,\n",
       " 'government': 241,\n",
       " 'ways': 242,\n",
       " 'find': 243,\n",
       " 'without': 244,\n",
       " 'live': 245,\n",
       " 'support': 246,\n",
       " 'making': 247,\n",
       " \"he's\": 248,\n",
       " 'top': 249,\n",
       " \"'\": 250,\n",
       " 'face': 251,\n",
       " 'give': 252,\n",
       " 'between': 253,\n",
       " 'john': 254,\n",
       " 'same': 255,\n",
       " 'lives': 256,\n",
       " \"you're\": 257,\n",
       " 'senate': 258,\n",
       " 'am': 259,\n",
       " '2': 260,\n",
       " 'getting': 261,\n",
       " 'three': 262,\n",
       " '000': 263,\n",
       " 'found': 264,\n",
       " '3': 265,\n",
       " \"won't\": 266,\n",
       " 'sexual': 267,\n",
       " 'shows': 268,\n",
       " 'fight': 269,\n",
       " 'presidential': 270,\n",
       " 'social': 271,\n",
       " 'college': 272,\n",
       " 'makes': 273,\n",
       " \"didn't\": 274,\n",
       " 'students': 275,\n",
       " 'feel': 276,\n",
       " 'food': 277,\n",
       " 'it’s': 278,\n",
       " 'gets': 279,\n",
       " 'report': 280,\n",
       " 'young': 281,\n",
       " 'times': 282,\n",
       " '2016': 283,\n",
       " 'always': 284,\n",
       " 'power': 285,\n",
       " 'north': 286,\n",
       " 'future': 287,\n",
       " 'vote': 288,\n",
       " 'keep': 289,\n",
       " 'democrats': 290,\n",
       " \"isn't\": 291,\n",
       " 'called': 292,\n",
       " 'call': 293,\n",
       " 'season': 294,\n",
       " 'use': 295,\n",
       " 'today': 296,\n",
       " 'deal': 297,\n",
       " '”': 298,\n",
       " 'violence': 299,\n",
       " 'few': 300,\n",
       " 'plan': 301,\n",
       " 'money': 302,\n",
       " 'under': 303,\n",
       " 'attack': 304,\n",
       " 'child': 305,\n",
       " 'away': 306,\n",
       " 'yet': 307,\n",
       " 'night': 308,\n",
       " 'lot': 309,\n",
       " 'comes': 310,\n",
       " 'case': 311,\n",
       " 'democratic': 312,\n",
       " 'dead': 313,\n",
       " 'gun': 314,\n",
       " 'both': 315,\n",
       " 'twitter': 316,\n",
       " 'bad': 317,\n",
       " 'shooting': 318,\n",
       " 'let': 319,\n",
       " 'free': 320,\n",
       " 'killed': 321,\n",
       " 'then': 322,\n",
       " 'calls': 323,\n",
       " 'important': 324,\n",
       " 'parents': 325,\n",
       " 'sanders': 326,\n",
       " 'become': 327,\n",
       " 'least': 328,\n",
       " 'start': 329,\n",
       " 'film': 330,\n",
       " 'united': 331,\n",
       " 'sex': 332,\n",
       " 'group': 333,\n",
       " '—': 334,\n",
       " \"that's\": 335,\n",
       " 'morning': 336,\n",
       " 'actually': 337,\n",
       " 'place': 338,\n",
       " 'congress': 339,\n",
       " 'community': 340,\n",
       " 'past': 341,\n",
       " 'hard': 342,\n",
       " 'each': 343,\n",
       " \"there's\": 344,\n",
       " 'talk': 345,\n",
       " 'job': 346,\n",
       " \"'the\": 347,\n",
       " 'race': 348,\n",
       " 'enough': 349,\n",
       " 'far': 350,\n",
       " 'win': 351,\n",
       " 'put': 352,\n",
       " 'debate': 353,\n",
       " 'justice': 354,\n",
       " 'game': 355,\n",
       " 'human': 356,\n",
       " 'believe': 357,\n",
       " 'behind': 358,\n",
       " 'baby': 359,\n",
       " 'supreme': 360,\n",
       " 'mom': 361,\n",
       " 'education': 362,\n",
       " 'washington': 363,\n",
       " 'once': 364,\n",
       " 'takes': 365,\n",
       " 'post': 366,\n",
       " 'everyone': 367,\n",
       " 'security': 368,\n",
       " 'federal': 369,\n",
       " 'set': 370,\n",
       " 'hope': 371,\n",
       " '4': 372,\n",
       " 'whether': 373,\n",
       " 'needs': 374,\n",
       " '7': 375,\n",
       " 'bernie': 376,\n",
       " 'coming': 377,\n",
       " 'open': 378,\n",
       " 'already': 379,\n",
       " 'music': 380,\n",
       " 'told': 381,\n",
       " 'trying': 382,\n",
       " 'wrong': 383,\n",
       " 'tell': 384,\n",
       " 'less': 385,\n",
       " 'policy': 386,\n",
       " 'doing': 387,\n",
       " 'second': 388,\n",
       " 'often': 389,\n",
       " 'ago': 390,\n",
       " 'tv': 391,\n",
       " 'leaders': 392,\n",
       " '6': 393,\n",
       " 'crisis': 394,\n",
       " 'trump’s': 395,\n",
       " 'run': 396,\n",
       " 'tax': 397,\n",
       " 'finally': 398,\n",
       " \"we're\": 399,\n",
       " 'someone': 400,\n",
       " 'russia': 401,\n",
       " 'art': 402,\n",
       " 'problem': 403,\n",
       " 'major': 404,\n",
       " 'pay': 405,\n",
       " 'month': 406,\n",
       " 'having': 407,\n",
       " 'working': 408,\n",
       " 'self': 409,\n",
       " 'team': 410,\n",
       " 'latest': 411,\n",
       " 'everything': 412,\n",
       " 'used': 413,\n",
       " 'together': 414,\n",
       " 'movie': 415,\n",
       " 'move': 416,\n",
       " 'company': 417,\n",
       " 'person': 418,\n",
       " 'office': 419,\n",
       " 'son': 420,\n",
       " 'ban': 421,\n",
       " 'matter': 422,\n",
       " 'left': 423,\n",
       " 'huffpost': 424,\n",
       " 'voters': 425,\n",
       " 'nothing': 426,\n",
       " 'body': 427,\n",
       " 'summer': 428,\n",
       " '2015': 429,\n",
       " 'james': 430,\n",
       " 'global': 431,\n",
       " 'taking': 432,\n",
       " 'moment': 433,\n",
       " 'looks': 434,\n",
       " 'took': 435,\n",
       " 'south': 436,\n",
       " 'different': 437,\n",
       " 'california': 438,\n",
       " 'administration': 439,\n",
       " 'others': 440,\n",
       " \"they're\": 441,\n",
       " 'friends': 442,\n",
       " 'months': 443,\n",
       " 'claims': 444,\n",
       " 'five': 445,\n",
       " 'photos': 446,\n",
       " 'act': 447,\n",
       " 'water': 448,\n",
       " 'series': 449,\n",
       " 'looking': 450,\n",
       " 'michael': 451,\n",
       " 'heart': 452,\n",
       " 'paul': 453,\n",
       " 'director': 454,\n",
       " 'living': 455,\n",
       " 'don’t': 456,\n",
       " 'hit': 457,\n",
       " '20': 458,\n",
       " 'texas': 459,\n",
       " 'true': 460,\n",
       " 'play': 461,\n",
       " 'age': 462,\n",
       " 'question': 463,\n",
       " 'perfect': 464,\n",
       " 'thought': 465,\n",
       " 'happy': 466,\n",
       " 'four': 467,\n",
       " 'candidate': 468,\n",
       " 'girl': 469,\n",
       " 'speech': 470,\n",
       " 'full': 471,\n",
       " 'mother': 472,\n",
       " 'super': 473,\n",
       " 'head': 474,\n",
       " 'daughter': 475,\n",
       " 'action': 476,\n",
       " 'judge': 477,\n",
       " 'words': 478,\n",
       " 'reportedly': 479,\n",
       " 'percent': 480,\n",
       " 'rise': 481,\n",
       " 'recent': 482,\n",
       " 'read': 483,\n",
       " 'muslim': 484,\n",
       " 'travel': 485,\n",
       " 'late': 486,\n",
       " 'meet': 487,\n",
       " 'decision': 488,\n",
       " 'military': 489,\n",
       " 'sure': 490,\n",
       " 'control': 491,\n",
       " 'questions': 492,\n",
       " 'name': 493,\n",
       " 'student': 494,\n",
       " 'accused': 495,\n",
       " 'cancer': 496,\n",
       " 'book': 497,\n",
       " 'shot': 498,\n",
       " '9': 499,\n",
       " 'leader': 500,\n",
       " 'system': 501,\n",
       " 'means': 502,\n",
       " 'easy': 503,\n",
       " 'across': 504,\n",
       " 'goes': 505,\n",
       " 'came': 506,\n",
       " 'try': 507,\n",
       " 'isis': 508,\n",
       " 'email': 509,\n",
       " '8': 510,\n",
       " 'fire': 511,\n",
       " 'university': 512,\n",
       " '11': 513,\n",
       " 'lost': 514,\n",
       " 'such': 515,\n",
       " 'message': 516,\n",
       " 'ready': 517,\n",
       " 'powerful': 518,\n",
       " 'officials': 519,\n",
       " 'host': 520,\n",
       " 'dog': 521,\n",
       " '2014': 522,\n",
       " 'lead': 523,\n",
       " 'marriage': 524,\n",
       " 'politics': 525,\n",
       " 'attacks': 526,\n",
       " 'among': 527,\n",
       " 'christmas': 528,\n",
       " 'senator': 529,\n",
       " 'leave': 530,\n",
       " 'seen': 531,\n",
       " 'stand': 532,\n",
       " \"i've\": 533,\n",
       " 'reason': 534,\n",
       " 'letter': 535,\n",
       " 'experience': 536,\n",
       " 'likely': 537,\n",
       " 'ryan': 538,\n",
       " 'stories': 539,\n",
       " 'reality': 540,\n",
       " 'order': 541,\n",
       " 'role': 542,\n",
       " 'probably': 543,\n",
       " 'special': 544,\n",
       " 'issues': 545,\n",
       " 'asked': 546,\n",
       " 'yourself': 547,\n",
       " 'almost': 548,\n",
       " 'short': 549,\n",
       " 'mean': 550,\n",
       " 'list': 551,\n",
       " 'assault': 552,\n",
       " 'point': 553,\n",
       " 'bring': 554,\n",
       " 'chief': 555,\n",
       " 'running': 556,\n",
       " 'facebook': 557,\n",
       " 'department': 558,\n",
       " 'middle': 559,\n",
       " 'secretary': 560,\n",
       " \"she's\": 561,\n",
       " 'immigration': 562,\n",
       " 'wanted': 563,\n",
       " 'early': 564,\n",
       " 'fox': 565,\n",
       " 'until': 566,\n",
       " 'kind': 567,\n",
       " 'female': 568,\n",
       " 'issue': 569,\n",
       " 'hate': 570,\n",
       " 'reasons': 571,\n",
       " 'photo': 572,\n",
       " 'talking': 573,\n",
       " 'small': 574,\n",
       " 'nearly': 575,\n",
       " 'girls': 576,\n",
       " 'record': 577,\n",
       " 'general': 578,\n",
       " 'learn': 579,\n",
       " 'street': 580,\n",
       " 'fear': 581,\n",
       " 'key': 582,\n",
       " \"aren't\": 583,\n",
       " 'idea': 584,\n",
       " 'march': 585,\n",
       " 'giving': 586,\n",
       " 'personal': 587,\n",
       " 'response': 588,\n",
       " 'russian': 589,\n",
       " 'save': 590,\n",
       " 'car': 591,\n",
       " 'lgbt': 592,\n",
       " 'ask': 593,\n",
       " 'florida': 594,\n",
       " 'fans': 595,\n",
       " 'obamacare': 596,\n",
       " 'international': 597,\n",
       " 'families': 598,\n",
       " 'instead': 599,\n",
       " \"women's\": 600,\n",
       " 'air': 601,\n",
       " 'went': 602,\n",
       " 'done': 603,\n",
       " 'step': 604,\n",
       " 'victims': 605,\n",
       " 'tweets': 606,\n",
       " 'workers': 607,\n",
       " 'turn': 608,\n",
       " 'interview': 609,\n",
       " 'trans': 610,\n",
       " 'mind': 611,\n",
       " 'learned': 612,\n",
       " 'iran': 613,\n",
       " '15': 614,\n",
       " 'possible': 615,\n",
       " 'china': 616,\n",
       " 'internet': 617,\n",
       " 'using': 618,\n",
       " 'schools': 619,\n",
       " 'pretty': 620,\n",
       " 'biggest': 621,\n",
       " 'secret': 622,\n",
       " 'single': 623,\n",
       " 'wall': 624,\n",
       " 'transgender': 625,\n",
       " 'holiday': 626,\n",
       " \"what's\": 627,\n",
       " 'nation': 628,\n",
       " 'yes': 629,\n",
       " 'gives': 630,\n",
       " 'thousands': 631,\n",
       " 'weekend': 632,\n",
       " '12': 633,\n",
       " 'despite': 634,\n",
       " 'kim': 635,\n",
       " 'cruz': 636,\n",
       " 'friday': 637,\n",
       " 'west': 638,\n",
       " 'service': 639,\n",
       " 'dad': 640,\n",
       " 'happened': 641,\n",
       " 'fact': 642,\n",
       " 'gender': 643,\n",
       " 'sleep': 644,\n",
       " 'korea': 645,\n",
       " 'culture': 646,\n",
       " '100': 647,\n",
       " 'break': 648,\n",
       " 'according': 649,\n",
       " 'number': 650,\n",
       " 'truth': 651,\n",
       " 'study': 652,\n",
       " 'rules': 653,\n",
       " 'based': 654,\n",
       " 'paris': 655,\n",
       " 'line': 656,\n",
       " 'governor': 657,\n",
       " 'remember': 658,\n",
       " 'inside': 659,\n",
       " 'legal': 660,\n",
       " 'though': 661,\n",
       " 'father': 662,\n",
       " 'anything': 663,\n",
       " 'science': 664,\n",
       " 'red': 665,\n",
       " 'sunday': 666,\n",
       " 'david': 667,\n",
       " 'abuse': 668,\n",
       " 'actor': 669,\n",
       " '2017': 670,\n",
       " 'drug': 671,\n",
       " 'share': 672,\n",
       " 'half': 673,\n",
       " 'fall': 674,\n",
       " 'seems': 675,\n",
       " 'clear': 676,\n",
       " 'chris': 677,\n",
       " 'force': 678,\n",
       " 'simple': 679,\n",
       " 'anyone': 680,\n",
       " 'lgbtq': 681,\n",
       " 'boy': 682,\n",
       " 'success': 683,\n",
       " 'close': 684,\n",
       " 'members': 685,\n",
       " 'online': 686,\n",
       " 'favorite': 687,\n",
       " 'saying': 688,\n",
       " 'nominee': 689,\n",
       " 'career': 690,\n",
       " 'challenge': 691,\n",
       " 'press': 692,\n",
       " 'energy': 693,\n",
       " 'plans': 694,\n",
       " 'side': 695,\n",
       " 'whole': 696,\n",
       " 'maybe': 697,\n",
       " 'companies': 698,\n",
       " 'peace': 699,\n",
       " 'official': 700,\n",
       " 'battle': 701,\n",
       " 'growing': 702,\n",
       " 'happen': 703,\n",
       " 'died': 704,\n",
       " 'daily': 705,\n",
       " 'ahead': 706,\n",
       " \"let's\": 707,\n",
       " 'worst': 708,\n",
       " 'risk': 709,\n",
       " 'common': 710,\n",
       " 'able': 711,\n",
       " 'nuclear': 712,\n",
       " 'groups': 713,\n",
       " 'economic': 714,\n",
       " 'known': 715,\n",
       " 'civil': 716,\n",
       " 'welcome': 717,\n",
       " '50': 718,\n",
       " 'syria': 719,\n",
       " 'front': 720,\n",
       " 'beautiful': 721,\n",
       " 'prison': 722,\n",
       " 'understand': 723,\n",
       " 'industry': 724,\n",
       " 'nfl': 725,\n",
       " 'candidates': 726,\n",
       " 'ad': 727,\n",
       " 'teen': 728,\n",
       " 'queer': 729,\n",
       " 'investigation': 730,\n",
       " 'class': 731,\n",
       " 'ted': 732,\n",
       " 'bush': 733,\n",
       " 'stay': 734,\n",
       " 'beyond': 735,\n",
       " 'movement': 736,\n",
       " 'officer': 737,\n",
       " 'following': 738,\n",
       " 'd': 739,\n",
       " 'longer': 740,\n",
       " 'heard': 741,\n",
       " 'stephen': 742,\n",
       " 'continue': 743,\n",
       " 'foreign': 744,\n",
       " 'sense': 745,\n",
       " 'tells': 746,\n",
       " 'fbi': 747,\n",
       " 'hours': 748,\n",
       " 'mike': 749,\n",
       " 'abortion': 750,\n",
       " 'research': 751,\n",
       " 'fighting': 752,\n",
       " 'attorney': 753,\n",
       " 'course': 754,\n",
       " 'meeting': 755,\n",
       " 'guy': 756,\n",
       " 'protect': 757,\n",
       " 'arrested': 758,\n",
       " 'sometimes': 759,\n",
       " 'cut': 760,\n",
       " 'jobs': 761,\n",
       " 'attention': 762,\n",
       " 'scott': 763,\n",
       " 'killing': 764,\n",
       " 'word': 765,\n",
       " 'finds': 766,\n",
       " \"obama's\": 767,\n",
       " 'six': 768,\n",
       " 'comments': 769,\n",
       " 'reveals': 770,\n",
       " 'king': 771,\n",
       " 'mental': 772,\n",
       " 'hear': 773,\n",
       " 'sign': 774,\n",
       " 'primary': 775,\n",
       " \"america's\": 776,\n",
       " 'hold': 777,\n",
       " 'soon': 778,\n",
       " 'forward': 779,\n",
       " 'myself': 780,\n",
       " 'healthy': 781,\n",
       " 'huge': 782,\n",
       " 'protest': 783,\n",
       " 'strong': 784,\n",
       " 'refugees': 785,\n",
       " 'outside': 786,\n",
       " '“i': 787,\n",
       " 'access': 788,\n",
       " 'c': 789,\n",
       " 'wins': 790,\n",
       " 'choice': 791,\n",
       " 'hollywood': 792,\n",
       " 'church': 793,\n",
       " 'pope': 794,\n",
       " 'himself': 795,\n",
       " 'planned': 796,\n",
       " 'process': 797,\n",
       " 'started': 798,\n",
       " 'reform': 799,\n",
       " 'religious': 800,\n",
       " 'countries': 801,\n",
       " 'fun': 802,\n",
       " 'later': 803,\n",
       " 'create': 804,\n",
       " 'talks': 805,\n",
       " 'program': 806,\n",
       " 'friend': 807,\n",
       " 'several': 808,\n",
       " 'iraq': 809,\n",
       " 'tips': 810,\n",
       " 'data': 811,\n",
       " \"you'll\": 812,\n",
       " 'tom': 813,\n",
       " 'actress': 814,\n",
       " 'happens': 815,\n",
       " 'i’m': 816,\n",
       " 'low': 817,\n",
       " 'freedom': 818,\n",
       " 'artist': 819,\n",
       " 'hill': 820,\n",
       " \"world's\": 821,\n",
       " 'wife': 822,\n",
       " 'calling': 823,\n",
       " 'mark': 824,\n",
       " 'couple': 825,\n",
       " 'relationship': 826,\n",
       " 'union': 827,\n",
       " 'society': 828,\n",
       " 'amid': 829,\n",
       " '30': 830,\n",
       " 'ceo': 831,\n",
       " 'executive': 832,\n",
       " 'including': 833,\n",
       " 'brown': 834,\n",
       " 'hot': 835,\n",
       " 'recently': 836,\n",
       " 'center': 837,\n",
       " 'rather': 838,\n",
       " 'san': 839,\n",
       " 'cops': 840,\n",
       " 'george': 841,\n",
       " 'local': 842,\n",
       " 'weeks': 843,\n",
       " 'lawmakers': 844,\n",
       " 'thinks': 845,\n",
       " 'opportunity': 846,\n",
       " 'millions': 847,\n",
       " 'chance': 848,\n",
       " 'allegedly': 849,\n",
       " 'east': 850,\n",
       " 'third': 851,\n",
       " 'threat': 852,\n",
       " 'serious': 853,\n",
       " 'changed': 854,\n",
       " 'trip': 855,\n",
       " \"wasn't\": 856,\n",
       " 'jimmy': 857,\n",
       " 'light': 858,\n",
       " 'knew': 859,\n",
       " 'town': 860,\n",
       " 'harassment': 861,\n",
       " 'final': 862,\n",
       " 'lose': 863,\n",
       " 'gave': 864,\n",
       " 'decades': 865,\n",
       " 'rep': 866,\n",
       " 'surprise': 867,\n",
       " 'guide': 868,\n",
       " 'football': 869,\n",
       " 'kill': 870,\n",
       " 'taken': 871,\n",
       " 'near': 872,\n",
       " 'missing': 873,\n",
       " 'stars': 874,\n",
       " 'visit': 875,\n",
       " 'rock': 876,\n",
       " 'co': 877,\n",
       " 'eat': 878,\n",
       " 'supporters': 879,\n",
       " 'amazing': 880,\n",
       " 'themselves': 881,\n",
       " 'given': 882,\n",
       " 'trailer': 883,\n",
       " 'birthday': 884,\n",
       " 'laws': 885,\n",
       " 'test': 886,\n",
       " 'speak': 887,\n",
       " 'singer': 888,\n",
       " 'wrote': 889,\n",
       " 'performance': 890,\n",
       " 'event': 891,\n",
       " 'rape': 892,\n",
       " 'economy': 893,\n",
       " 'conservative': 894,\n",
       " 'billion': 895,\n",
       " 'joe': 896,\n",
       " 'continues': 897,\n",
       " 'voting': 898,\n",
       " 'lessons': 899,\n",
       " 'oil': 900,\n",
       " 'carolina': 901,\n",
       " 'birth': 902,\n",
       " 'won': 903,\n",
       " 'term': 904,\n",
       " 'sports': 905,\n",
       " 'trade': 906,\n",
       " 'focus': 907,\n",
       " 'cover': 908,\n",
       " 'god': 909,\n",
       " 'turned': 910,\n",
       " 'else': 911,\n",
       " 'became': 912,\n",
       " 'medical': 913,\n",
       " 'israel': 914,\n",
       " 'beauty': 915,\n",
       " 'thinking': 916,\n",
       " 'style': 917,\n",
       " 'racist': 918,\n",
       " 'mass': 919,\n",
       " 'song': 920,\n",
       " 'k': 921,\n",
       " 'seem': 922,\n",
       " 'sen': 923,\n",
       " 'answer': 924,\n",
       " 'room': 925,\n",
       " 'cannot': 926,\n",
       " 'needed': 927,\n",
       " 'met': 928,\n",
       " 'dream': 929,\n",
       " 'safe': 930,\n",
       " 'space': 931,\n",
       " 'along': 932,\n",
       " 'hair': 933,\n",
       " 'worth': 934,\n",
       " \"you've\": 935,\n",
       " 'within': 936,\n",
       " 'celebrate': 937,\n",
       " 'held': 938,\n",
       " 'voter': 939,\n",
       " 'claim': 940,\n",
       " 'evidence': 941,\n",
       " 'helped': 942,\n",
       " 'ex': 943,\n",
       " 'loss': 944,\n",
       " 'pick': 945,\n",
       " 'release': 946,\n",
       " 'steps': 947,\n",
       " 'thanks': 948,\n",
       " 'information': 949,\n",
       " 'pro': 950,\n",
       " 'advice': 951,\n",
       " 'winning': 952,\n",
       " 'released': 953,\n",
       " 'conversation': 954,\n",
       " 'instagram': 955,\n",
       " 'july': 956,\n",
       " 'cities': 957,\n",
       " 'feeling': 958,\n",
       " 'un': 959,\n",
       " 'led': 960,\n",
       " 'hundreds': 961,\n",
       " 'moms': 962,\n",
       " 'alone': 963,\n",
       " 'faces': 964,\n",
       " 'colbert': 965,\n",
       " 'turns': 966,\n",
       " 'taylor': 967,\n",
       " 'building': 968,\n",
       " 'return': 969,\n",
       " 'defense': 970,\n",
       " 'monday': 971,\n",
       " 'truly': 972,\n",
       " 'kid': 973,\n",
       " 'fan': 974,\n",
       " 'toward': 975,\n",
       " 'financial': 976,\n",
       " 'festival': 977,\n",
       " 'impact': 978,\n",
       " 'felt': 979,\n",
       " 'awards': 980,\n",
       " 'paid': 981,\n",
       " '13': 982,\n",
       " 'equality': 983,\n",
       " 'murder': 984,\n",
       " 'tuesday': 985,\n",
       " 'leadership': 986,\n",
       " 'winter': 987,\n",
       " 'suspect': 988,\n",
       " 'tried': 989,\n",
       " 'push': 990,\n",
       " 'color': 991,\n",
       " 'simply': 992,\n",
       " 'syrian': 993,\n",
       " 'however': 994,\n",
       " 'road': 995,\n",
       " 'hand': 996,\n",
       " 'fashion': 997,\n",
       " 'especially': 998,\n",
       " 'j': 999,\n",
       " 'ice': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras.tokenizer allows us to turn each unique word in our all_text list into a unique integer.since ML model deals with integers not words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "all_text = np.array(all_text)\n",
    "label_list = np.array(label_list)\n",
    "X_train,X_test,y_train,y_test = train_test_split(all_text,label_list,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset and labels into training and testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tokens = tokenizer.texts_to_sequences(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now convert the lists in all_text to sequences of integers of their corresponding words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LISTEN: This Is What The Sun 'Sounds' Like [1324, 16, 7, 33, 1, 2553, 96001, 57]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[15000],\n",
    "x_train_tokens[15000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_tokens = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the lists are of different lengths .To feed in our model , we have to make all our lists of same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = [len(tokens) for tokens in x_train_tokens + x_test_tokens]\n",
    "num_tokens = np.array(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.83365736184784"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "248"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9446111257790686"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(num_tokens < max_tokens) / len(num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now pad small length lists and truncate long lists , so that all lists are of 54 length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tflearn.data_utils import pad_sequences,to_categorical\n",
    "x_train_pad = pad_sequences(x_train_tokens,maxlen=max_tokens)\n",
    "x_test_pad = pad_sequences(x_test_tokens,maxlen=max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87492, 54)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37497, 54)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = tokenizer.word_index\n",
    "inverse_map = dict(zip(idx.values(), idx.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_string(tokens):\n",
    "    # Map from tokens back to words.\n",
    "    words = [inverse_map[token] for token in tokens if token != 0]\n",
    "    \n",
    "    # Concatenate all words.\n",
    "    text = \" \".join(words)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99572"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index) #total no. of words in our vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sitting is the new smoking and a hazard to your career health'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_to_string(x_train_tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1931, 7, 1, 27, 3259, 5, 3, 12470, 2, 30, 690, 122]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_tokens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad = 'pre'\n",
    "#x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens,\n",
    "#                           padding=pad, truncating=pad)\n",
    "#x_test_pad = pad_sequences(x_test_tokens, maxlen=max_tokens,\n",
    "#                           padding=pad, truncating=pad)\n",
    "#model = Sequential()\n",
    "#embedding_size = 128\n",
    "#model.add(Embedding(input_dim=len(tokenizer.word_index),\n",
    "#                    output_dim=embedding_size,\n",
    "#                    input_length=max_tokens,\n",
    "#                    name='layer_embedding'))\n",
    "#optimizer = Adam(lr=1e-3)\n",
    "#model.add(LSTM(128,dropout=0.2))\n",
    "#model.add(Dense(31, activation='softmax'))\n",
    "#model.compile(loss='categorical_crossentropy',\n",
    "#              optimizer='adam',\n",
    "#              metrics=['accuracy'])\n",
    "#model.summary()\n",
    "#model.fit(x_train_pad, y_train,\n",
    "#          epochs=3, batch_size=64)\n",
    "#score = model.evaluate(x_test_pad, y_test, batch_size=64)\n",
    "#print(\"Accuracy: {0:.2%}\".format(score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is keras model and below is tflearn model u can choose either , better accuracy with tflearn , i think so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tflearn\n",
    "tf.reset_default_graph()\n",
    "RNN = tflearn.input_data([None,54])\n",
    "RNN = tflearn.embedding(RNN,input_dim=99573,output_dim=128)\n",
    "RNN = tflearn.lstm(RNN,128,dropout=0.8)\n",
    "RNN = tflearn.fully_connected(RNN,31,activation='softmax')\n",
    "RNN = tflearn.regression(RNN,optimizer='adam',learning_rate=0.001,loss='categorical_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 13674  | total loss: \u001b[1m\u001b[32m0.39250\u001b[0m\u001b[0m | time: 591.191s\n",
      "| Adam | epoch: 005 | loss: 0.39250 - acc: 0.8829 -- iter: 87488/87492\n",
      "Training Step: 13675  | total loss: \u001b[1m\u001b[32m0.42521\u001b[0m\u001b[0m | time: 613.890s\n",
      "| Adam | epoch: 005 | loss: 0.42521 - acc: 0.8790 | val_loss: 1.85761 - val_acc: 0.5730 -- iter: 87492/87492\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = tflearn.DNN(RNN,tensorboard_verbose=0)\n",
    "model.fit(x_train_pad,y_train,n_epoch =5,validation_set=(x_test_pad,y_test),show_metric=True,batch_size=32,run_id='huff')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change tensorboard_verbose = 3 if you want to visualise using tensorboard and dont increase n_epochs >5 ,model may overfit.The input_dim in \n",
    "tflearn is len(tokenizer.word_index) + 1 compared to keras which is len(tokenizer.word_index) i.e 99572.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:C:\\python36\\huff\\quickest.model is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "model.save('quickest.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelo = {'POLITICS': 1,\n",
    "'ENTERTAINMENT': 2,\n",
    "'HEALTHY LIVING': 3,\n",
    "'QUEER VOICES': 4,\n",
    "'BUSINESS': 5,\n",
    "'SPORTS': 6,\n",
    "'COMEDY': 7,\n",
    "'PARENTS': 8,\n",
    "'BLACK VOICES': 9,\n",
    "'THE WORLDPOST': 10,\n",
    "'WOMEN': 11,\n",
    "'CRIME': 12,\n",
    "'MEDIA': 13,\n",
    "'WEIRD NEWS': 14,\n",
    "'GREEN': 15,\n",
    "'IMPACT': 16,\n",
    "'WORLDPOST': 17,\n",
    "'RELIGION': 18,\n",
    "'STYLE': 19,\n",
    "'WORLD NEWS': 20,\n",
    "'TRAVEL': 21,\n",
    "'TASTE': 22,\n",
    "'ARTS': 23,\n",
    "'FIFTY': 24,\n",
    "'GOOD NEWS': 25,\n",
    "'SCIENCE': 26,\n",
    "'ARTS & CULTURE': 27,\n",
    "'TECH': 28,\n",
    "'COLLEGE': 29,\n",
    "'LATINO VOICES': 30,\n",
    "'EDUCATION': 31}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictcategory(user_input):\n",
    "    \n",
    "    sentence  = tokenizer.texts_to_sequences(user_input)\n",
    "    sentence = pad_sequences(sentence,maxlen=max_tokens)\n",
    "    result = model.predict(sentence)\n",
    "    result = np.argmax(result)\n",
    "    result += 1\n",
    "    for f in labelo:\n",
    "        if labelo.get(f) == result:\n",
    "            answer = f\n",
    "    return answer\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "yes = predictcategory(['books for graduation schools will provide'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EDUCATION'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
